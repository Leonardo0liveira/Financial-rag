from langchain_core.tools import tool
import time
import chromadb
from chromadb.config import Settings
from typing import List, Dict
from pathlib import Path
import os

# Configurar tokenizers para evitar warnings
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ConfiguraÃ§Ãµes
CHROMADB_PATH = "./chromadb_storage"
COLLECTION_NAME = "financial_reports"

class SimpleVectorDB:
    """Banco de vetores simplificado usando ChromaDB."""
    
    def __init__(self):
        """Inicializa o cliente ChromaDB."""
        Path(CHROMADB_PATH).mkdir(exist_ok=True)
        
        self.client = chromadb.PersistentClient(
            path=CHROMADB_PATH,
            settings=Settings(anonymized_telemetry=False, allow_reset=True)
        )
        
        # Criar/carregar coleÃ§Ã£o
        try:
            self.collection = self.client.get_collection(
                name=COLLECTION_NAME,
                embedding_function=chromadb.utils.embedding_functions.DefaultEmbeddingFunction()
            )
            print(f"ğŸ“š ColeÃ§Ã£o carregada: {self.collection.count()} documentos")
        except:
            self.collection = self.client.create_collection(
                name=COLLECTION_NAME, 
                embedding_function=chromadb.utils.embedding_functions.DefaultEmbeddingFunction()
            )
            print(f"ğŸ“š Nova coleÃ§Ã£o criada")
    
    def add_documents(self, documents: List[str]) -> Dict:
        """Adiciona documentos Ã  coleÃ§Ã£o, dividindo em chunks se necessÃ¡rio."""
        try:
            existing_count = self.collection.count()
            all_chunks = []
            all_ids = []
            
            for i, doc in enumerate(documents):
                # Se documento Ã© muito grande (>10k chars), dividir em chunks
                if len(doc) > 10000:
                    chunks = self._split_into_chunks(doc)
                    for j, chunk in enumerate(chunks):
                        all_chunks.append(chunk)
                        all_ids.append(f"doc_{existing_count + i}_chunk_{j}")
                else:
                    all_chunks.append(doc)
                    all_ids.append(f"doc_{existing_count + i}")
            
            self.collection.add(documents=all_chunks, ids=all_ids)
            total_docs = self.collection.count()
            
            print(f"âœ… {len(all_chunks)} chunks adicionados. Total: {total_docs}")
            return {"status": "success", "documents_added": len(all_chunks), "total_documents": total_docs}
            
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def _split_into_chunks(self, document: str, chunk_size: int = 2000, overlap: int = 200) -> List[str]:
        """Divide um documento grande em chunks menores com sobreposiÃ§Ã£o."""
        chunks = []
        
        # Extrair tÃ­tulo do documento se existir
        lines = document.split('\n')
        title = ""
        content_start = 0
        
        for i, line in enumerate(lines[:5]):
            if 'ğŸ“„' in line:
                title = line.strip()
                content_start = i + 1
                break
        
        # Juntar o conteÃºdo restante
        content = '\n'.join(lines[content_start:])
        
        # Dividir em chunks
        start = 0
        chunk_num = 0
        
        while start < len(content):
            end = start + chunk_size
            
            # Se nÃ£o Ã© o Ãºltimo chunk, tentar quebrar em uma linha completa
            if end < len(content):
                # Procurar por quebra de linha prÃ³xima
                for i in range(end, max(start + chunk_size//2, end - 200), -1):
                    if content[i] == '\n':
                        end = i
                        break
            
            chunk_content = content[start:end].strip()
            
            if chunk_content:
                # Adicionar tÃ­tulo se existir + nÃºmero do chunk
                if title:
                    chunk_title = f"{title} (Parte {chunk_num + 1})"
                else:
                    chunk_title = f"Documento (Parte {chunk_num + 1})"
                
                full_chunk = f"{chunk_title}:\n{chunk_content}"
                chunks.append(full_chunk)
                chunk_num += 1
            
            # PrÃ³ximo chunk com sobreposiÃ§Ã£o
            start = end - overlap if end > overlap else end
            
            # Evitar loop infinito
            if start >= len(content):
                break
        
        return chunks
    
    def search(self, query: str, k: int = 3) -> List[Dict]:
        """Busca e retorna os chunks mais relevantes."""
        try:
            results = self.collection.query(query_texts=[query], n_results=k)
            
            chunks = []
            if results['documents'] and results['documents'][0]:
                for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):
                    similarity = 1.0 / (1.0 + distance)  # Converter distÃ¢ncia para similaridade
                    chunks.append({
                        "content": doc,
                        "similarity": similarity,
                        "rank": i + 1
                    })
            
            return chunks
            
        except Exception as e:
            print(f"Erro na busca: {e}")
            return []
    
    def get_stats(self) -> Dict:
        """Retorna estatÃ­sticas do banco."""
        return {
            "total_documents": self.collection.count(),
            "collection_name": COLLECTION_NAME,
            "storage_path": CHROMADB_PATH
        }
    
    def clear_collection(self) -> Dict:
        """Limpa todos os documentos da coleÃ§Ã£o atual."""
        try:
            # Pegar todos os IDs
            all_docs = self.collection.get()
            if all_docs['ids']:
                self.collection.delete(ids=all_docs['ids'])
                return {"status": "success", "message": f"Removidos {len(all_docs['ids'])} documentos"}
            else:
                return {"status": "info", "message": "ColeÃ§Ã£o jÃ¡ estava vazia"}
        except Exception as e:
            return {"status": "error", "message": f"Erro ao limpar coleÃ§Ã£o: {str(e)}"}
    
    def reset_database(self) -> Dict:
        """Reseta completamente o banco de dados (remove tudo)."""
        try:
            # Deletar coleÃ§Ã£o
            self.client.delete_collection(COLLECTION_NAME)
            
            # Recriar coleÃ§Ã£o vazia
            self.collection = self.client.create_collection(
                name=COLLECTION_NAME,
                embedding_function=chromadb.utils.embedding_functions.DefaultEmbeddingFunction()
            )
            
            return {"status": "success", "message": "Banco de dados resetado completamente"}
        except Exception as e:
            return {"status": "error", "message": f"Erro ao resetar banco: {str(e)}"}

# InstÃ¢ncia global
vector_db = SimpleVectorDB()

@tool
def vectorize_financial_reports(reports: List[str]) -> Dict:
    """Indexa relatÃ³rios financeiros no banco de vetores."""
    if not reports:
        return {"status": "error", "message": "Nenhum relatÃ³rio fornecido"}
    return vector_db.add_documents(reports)

@tool  
def semantic_search(query: str, k: int = 3) -> List[Dict]:
    """Realiza busca semÃ¢ntica nos relatÃ³rios financeiros."""
    if not query or not query.strip():
        return []
    return vector_db.search(query, k)

@tool
def get_vector_stats() -> Dict:
    """Retorna estatÃ­sticas do banco de vetores."""
    return vector_db.get_stats()

@tool
def get_retrieval_metrics() -> Dict:
    """Retorna mÃ©tricas do sistema de recuperaÃ§Ã£o."""
    stats = vector_db.get_stats()
    return {
        "total_documents": stats["total_documents"],
        "collection_name": stats["collection_name"], 
        "storage_path": stats["storage_path"],
        "status": "active" if stats["total_documents"] > 0 else "empty"
    }

@tool
def clear_vector_database() -> Dict:
    """
    Limpa todos os documentos do banco vetorial.
    
    Remove todos os documentos indexados, mantendo a estrutura do banco.
    Use quando quiser recomeÃ§ar com documentos novos.
    
    Returns:
        Status da operaÃ§Ã£o de limpeza
    """
    return vector_db.clear_collection()

@tool  
def reset_vector_database() -> Dict:
    """
    Reseta completamente o banco vetorial.
    
    Remove a coleÃ§Ã£o inteira e recria do zero.
    Use quando houver problemas de configuraÃ§Ã£o ou corrupÃ§Ã£o.
    
    Returns:
        Status da operaÃ§Ã£o de reset
    """
    return vector_db.reset_database()

def extract_relevant_info(document: str, query: str) -> str:
    """
    Extrai informaÃ§Ãµes relevantes do documento baseado na query do usuÃ¡rio usando busca semÃ¢ntica.
    
    Args:
        document: Documento completo encontrado
        query: Pergunta do usuÃ¡rio
        
    Returns:
        InformaÃ§Ãµes especÃ­ficas extraÃ­das
    """
    try:
        # Dividir documento em chunks menores para anÃ¡lise
        document_lines = [line.strip() for line in document.split('\n') if line.strip()]
        
        # Extrair tÃ­tulo do documento
        document_title = ""
        for line in document_lines[:3]:
            if 'ğŸ“„' in line or any(word in line.lower() for word in [ 'relatÃ³rio', 'trimestre']):
                document_title = f"**{line}**"
                break
        
        # Extrair termos-chave da query do usuÃ¡rio para busca flexÃ­vel
        query_terms = set()
        query_lower = query.lower()
        
        # Adicionar palavras da query (removendo stop words bÃ¡sicas)
        stop_words = {'o', 'a', 'do', 'da', 'de', 'no', 'na', 'em', 'por', 'para', 'com', 'foi', 'ser', 'qual', 'que', 'como'}
        for word in query_lower.split():
            cleaned_word = word.strip('.,?!();:')
            if len(cleaned_word) > 2 and cleaned_word not in stop_words:
                query_terms.add(cleaned_word)
        
        # Scoring de linhas baseado na relevÃ¢ncia para a query
        scored_lines = []
        
        for line in document_lines:
            if len(line) < 10:  # Ignorar linhas muito curtas
                continue
                
            line_lower = line.lower()
            score = 0
            
            # PontuaÃ§Ã£o por termos da query encontrados
            for term in query_terms:
                if term in line_lower:
                    score += 3
                    
            # BonificaÃ§Ã£o para linhas com valores financeiros
            if any(indicator in line_lower for indicator in ['r$', 'milhÃµes', 'bilhÃµes', '%']):
                score += 2
                
            # BonificaÃ§Ã£o para linhas com mÃ©tricas financeiras
            if any(metric in line_lower for metric in ['lucro', 'receita', 'ebitda', 'roe', 'margem', 'patrimÃ´nio']):
                score += 2
                
            # BonificaÃ§Ã£o para linhas com nÃºmeros e perÃ­odos
            if any(period in line_lower for period in ['3t25', 'q3', 'trimestre', '2024', '2025']):
                score += 1
                
            if score > 0:
                scored_lines.append((score, line))
        
        # Ordenar por score e pegar as mais relevantes
        scored_lines.sort(key=lambda x: x[0], reverse=True)
        relevant_lines = [f"- {line}" for score, line in scored_lines[:8]]  # Top 8 linhas mais relevantes
        
        # Se nÃ£o encontrou linhas relevantes, usar fallback inteligente
        if not relevant_lines:
            # Buscar linhas com dados financeiros gerais
            for line in document_lines:
                if any(indicator in line.lower() for indicator in ['r$', '%', 'milhÃµes', 'bilhÃµes']):
                    relevant_lines.append(f"- {line}")
                    if len(relevant_lines) >= 5:
                        break
        
        # Se ainda nÃ£o encontrou nada, pegar inÃ­cio do documento
        if not relevant_lines:
            for i, line in enumerate(document_lines[1:6]):  # Pular tÃ­tulo
                if len(line) > 20:
                    relevant_lines.append(f"- {line}")
        
        # Montar resultado final
        result_parts = []
        
        if document_title:
            result_parts.append(document_title)
            result_parts.append("")
        
        if relevant_lines:
            result_parts.extend(relevant_lines)
        else:
            # Ãšltimo recurso
            result_parts.append(f"- {document[:300]}...")
        
        return "\n".join(result_parts)
        
    except Exception as e:
        # Em caso de erro, retornar versÃ£o truncada
        return f"**Erro na extraÃ§Ã£o:** {str(e)}\n\n{document[:300]}..."
        
    except Exception as e:
        # Em caso de erro, retornar versÃ£o truncada
        return document[:300] + "..."

def read_file_content(file_path: str) -> str:
    """
    LÃª conteÃºdo de diferentes tipos de arquivo.
    
    Args:
        file_path: Caminho para o arquivo
        
    Returns:
        ConteÃºdo do arquivo como string
    """
    try:
        file_path = Path(file_path)
        
        if file_path.suffix.lower() == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
                
        elif file_path.suffix.lower() == '.md':
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
                
        elif file_path.suffix.lower() == '.pdf':
            try:
                import PyPDF2
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    text = ""
                    for page in reader.pages:
                        extracted = page.extract_text()
                        if extracted:  # Verificar se extraiu texto
                            text += extracted + "\n"
                    return text if text.strip() else "âš ï¸ NÃ£o foi possÃ­vel extrair texto do PDF"
            except ImportError:
                return f"âš ï¸ PyPDF2 nÃ£o instalado. Para processar PDFs: pip install PyPDF2"
            except Exception as e:
                return f"âš ï¸ Erro ao processar PDF: {str(e)}"
                
        elif file_path.suffix.lower() in ['.docx', '.doc']:
            try:
                import docx
                doc = docx.Document(file_path)
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                return text
            except ImportError:
                return f"âš ï¸ python-docx nÃ£o instalado. Para processar Word: pip install python-docx"
                
        else:
            return f"âŒ Formato de arquivo nÃ£o suportado: {file_path.suffix}"
            
    except Exception as e:
        return f"âŒ Erro ao ler arquivo {file_path}: {str(e)}"

@tool
def index_documents_from_path(folder_path: str, file_pattern: str = "*.txt") -> Dict:
    """
    Indexa documentos de uma pasta especÃ­fica.
    
    Args:
        folder_path: Caminho para a pasta com documentos
        file_pattern: PadrÃ£o de arquivos (ex: "*.txt", "*.pdf", "*.md")
        
    Returns:
        Resultado da indexaÃ§Ã£o
    """
    try:
        folder = Path(folder_path)
        
        if not folder.exists():
            return {"status": "error", "message": f"Pasta nÃ£o encontrada: {folder_path}"}
            
        if not folder.is_dir():
            return {"status": "error", "message": f"Caminho nÃ£o Ã© uma pasta: {folder_path}"}
        
        # Encontrar arquivos
        files = list(folder.glob(file_pattern))
        
        if not files:
            return {"status": "error", "message": f"Nenhum arquivo encontrado com padrÃ£o '{file_pattern}' em {folder_path}"}
        
        # Ler conteÃºdo dos arquivos
        documents = []
        for file_path in files:
            content = read_file_content(file_path)
            documents.append(f"ğŸ“„ {file_path.name}:\n{content}")
        
        # Indexar no banco vetorial
        result = vector_db.add_documents(documents)
        
        return {
            "status": result["status"] if "status" in result else "success",
            "files_processed": len(files),
            "documents_added": result.get("documents_added", len(documents)),
            "total_documents": result.get("total_documents", 0),
            "files": [f.name for f in files]
        }
        
    except Exception as e:
        return {"status": "error", "message": str(e)}

@tool
def financial_reports_retriever_tool(query: str) -> str:
    """
    Retriever direto para relatÃ³rios financeiros.
    
    Args:
        query: Pergunta sobre dados financeiros
        
    Returns:
        Chunks mais relevantes encontrados
    """
    try:
        # Buscar chunks relevantes
        chunks = vector_db.search(query, k=3)
        
        if not chunks:
            return f"""**âŒ Nenhum resultado encontrado para:** "{query}"

**SugestÃµes:**
- Carregue documentos usando a interface Streamlit  
- Tente termos como "lucro", "receita", "patrimÃ´nio"
- Verifique se hÃ¡ PDFs indexados no sistema"""

        # Pegar o melhor chunk
        best_chunk = chunks[0]
        
        # Limitar o tamanho do conteÃºdo
        content = best_chunk["content"]
        if len(content) > 1500:
            content = content[:1500] + "..."
            
        return f"""**ğŸ“Š InformaÃ§Ã£o Encontrada**

**Similaridade:** {best_chunk['similarity']:.1%}

---

{content}

---
*Retriever: ChromaDB com embeddings*"""
        
    except Exception as e:
        return f"âŒ Erro no retriever: {str(e)}"